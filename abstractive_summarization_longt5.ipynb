{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d4c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max checkpoint path: ./results/checkpoint-181000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the main directory containing checkpoints\n",
    "base_dir = \"results\"\n",
    "folders = os.listdir(base_dir)\n",
    "\n",
    "# Filter for checkpoint folders and extract their numbers\n",
    "checkpoints = [\n",
    "    (folder, int(re.search(r\"checkpoint-(\\d+)\", folder).group(1)))\n",
    "    for folder in folders\n",
    "    if re.match(r\"checkpoint-\\d+\", folder)\n",
    "]\n",
    "\n",
    "# Get the folder with the highest checkpoint number\n",
    "max_checkpoint = max(checkpoints, key=lambda x: x[1])[0]\n",
    "\n",
    "checkpoint_path = \"./results/\" + max_checkpoint\n",
    "print(\"Max checkpoint path:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335148c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c1e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . \n",
      " it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models\n",
      "Summary: additive models play an important role in semiparametric statistics . \n",
      " this paper gives learning rates for regularized kernel based methods for additive models . \n",
      " these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . \n",
      " additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .    * \n",
      " key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load arXiv summarization dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"Article:\", sample[\"article\"][:500])\n",
    "print(\"Summary:\", sample[\"abstract\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b272b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run\n",
    "# from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "# model_name = \"google/long-t5-tglobal-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = LongT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# model.config.use_cache = False\n",
    "# model.gradient_checkpointing_enable() \n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True  # avoid shape mismatch crashes\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddd5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    inputs = [\"summarize: \" + doc for doc in batch[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=4096, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"abstract\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65810c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run\n",
    "# Assuming your dataset has a 'train' split\n",
    "# tokenized_dataset = dataset[\"train\"].map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"article\", \"abstract\"]\n",
    "# )\n",
    "# 53 minutes\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# tokenized_dataset.save_to_disk(\"tokenized_dataset\")\n",
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(\"tokenized_dataset\")\n",
    "\n",
    "# Then perform train-test split on the tokenized dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c8d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    accumulated_fmeasures = {\n",
    "        'rouge1': 0.0,\n",
    "        'rouge2': 0.0,\n",
    "        'rougeL': 0.0, \n",
    "    }\n",
    "    num_samples = 0\n",
    "\n",
    "    for pred_text, ref_text in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(ref_text, pred_text)\n",
    "\n",
    "        for key in accumulated_fmeasures:\n",
    "            if key in scores:\n",
    "                accumulated_fmeasures[key] += scores[key].fmeasure\n",
    "        num_samples += 1\n",
    "\n",
    "    average_metrics = {}\n",
    "    if num_samples > 0:\n",
    "        for key, total_fmeasure in accumulated_fmeasures.items():\n",
    "            average_metrics[key] = (total_fmeasure / num_samples) * 100\n",
    "\n",
    "    return average_metrics\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fead318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0)) \n",
    "print(torch.backends.cudnn.version()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2680d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "        print(\"Cached:   \", torch.cuda.memory_reserved() / 1024**3, \"GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available.\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831788b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    # eval_steps=5000,                     \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,                      \n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,       \n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    # fp16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    bf16=False,                          # Use bfloat16 for better performance on A100 GPUs\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,\n",
    "    disable_tqdm=False,                   # Enable tqdm to check live logs\n",
    "    report_to=[],                         # Avoid WandB etc.\n",
    "    save_safetensors=True,                # Save in safer format\n",
    ")\n",
    "\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=1000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=1000, \n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"rougeL\",\n",
    "#     greater_is_better=True,\n",
    "#     learning_rate=3e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     eval_accumulation_steps=4, \n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=200,\n",
    "\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     max_grad_norm=1.0,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e89a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class ClipNanGradientsCallback(TrainerCallback):    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if model is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eca455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    "    callbacks=[ClipNanGradientsCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66df9c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\trainer.py:3017: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/182732 [00:00<?, ?it/s]c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\trainer.py:2758: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 181200/182732 [09:19<1:13:03,  2.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5613, 'grad_norm': 1.0089662075042725, 'learning_rate': 8.383862706039447e-08, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 181400/182732 [18:51<1:03:22,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6676, 'grad_norm': 1.443673849105835, 'learning_rate': 7.289363658253618e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 181600/182732 [28:21<53:15,  2.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6064, 'grad_norm': 1.3262107372283936, 'learning_rate': 6.194864610467789e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 181800/182732 [37:52<44:20,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6021, 'grad_norm': 1.496598482131958, 'learning_rate': 5.100365562681961e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 182000/182732 [47:23<34:48,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6008, 'grad_norm': 1.7560162544250488, 'learning_rate': 4.005866514896132e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 182200/182732 [57:22<25:35,  2.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6195, 'grad_norm': 1.6497410535812378, 'learning_rate': 2.911367467110304e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 182400/182732 [1:06:55<15:46,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6291, 'grad_norm': 1.2874150276184082, 'learning_rate': 1.8168684193244754e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 182600/182732 [1:16:27<06:16,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6305, 'grad_norm': 1.576964259147644, 'learning_rate': 7.223693715386468e-09, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182732/182732 [1:22:42<00:00, 36.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4962.859, 'train_samples_per_second': 73.64, 'train_steps_per_second': 36.82, 'train_loss': 0.015285461631534245, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=182732, training_loss=0.015285461631534245, metrics={'train_runtime': 4962.859, 'train_samples_per_second': 73.64, 'train_steps_per_second': 36.82, 'total_flos': 2.002116063264768e+18, 'train_loss': 0.015285461631534245, 'epoch': 1.999994527534709})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial run\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be41bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./longt5_best_model\\\\tokenizer_config.json',\n",
       " './longt5_best_model\\\\special_tokens_map.json',\n",
       " './longt5_best_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.save_model(\"./longt5_best_model\")\n",
    "tokenizer.save_pretrained(\"./longt5_best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a30c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Summary:\n",
      " in this paper the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from 1923 - 1933 ( cycle 16 ) is considered. the daily sunspot areas, the mean sunspot areas per carrington rotation, the monthly sunspot numbers and their fluctuations, which are obtained after removing the 11-year cycle are analysed. the power spectrum method is used for the diagnosis of the reasons of the existence of peaks, which are obtained by the fast fourier transformation algorithm with the hamming window function and the blackman - tukey power spectrum method. numerical results of the new method of the diagnosis of an echo - effect in the power spectrum are presented.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"./longt5_best_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./longt5_best_model\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input\n",
    "text = dataset[\"test\"][0][\"article\"]\n",
    "input_text = \"summarize: \" + text\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=4096,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Move input to the same device as model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=256,\n",
    "    min_length=30,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Summary:\\n\", summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d35224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:46<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores on Test Set:\n",
      "rouge1: 0.4118\n",
      "rouge2: 0.1619\n",
      "rougeL: 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\", split=\"test\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"./longt5_best_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./longt5_best_model\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Evaluation loop\n",
    "n_samples = 100  # You can increase to 100 or full len(dataset) -> 6440 for full evaluation\n",
    "scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "for i in tqdm(range(n_samples), desc=\"Evaluating\"):\n",
    "    article = dataset[i][\"article\"]\n",
    "    reference = dataset[i][\"abstract\"]\n",
    "\n",
    "    input_text = \"summarize: \" + article\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=4096, truncation=True).to(device)\n",
    "\n",
    "    # summary_ids = model.generate(\n",
    "    #     inputs[\"input_ids\"],\n",
    "    #     max_length=256,\n",
    "    #     min_length=30,\n",
    "    #     length_penalty=2.0,\n",
    "    #     num_beams=4,\n",
    "    #     early_stopping=True\n",
    "    # )\n",
    "\n",
    "    # summary_ids = model.generate(\n",
    "    #     inputs[\"input_ids\"],\n",
    "    #     max_length=400,           # allow more detail\n",
    "    #     min_length=50,            # ensure enough content\n",
    "    #     length_penalty=1.0,       # less penalty for longer summaries\n",
    "    #     num_beams=6,              # explore more beam candidates\n",
    "    #     no_repeat_ngram_size=3,   # avoid repetitive phrases\n",
    "    #     early_stopping=True\n",
    "    # )   \n",
    "\n",
    "    summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=256,\n",
    "    min_length=30,\n",
    "    length_penalty=1.8,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    predicted = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    score = scorer.score(reference, predicted)\n",
    "    for key in scores:\n",
    "        scores[key].append(score[key].fmeasure)\n",
    "\n",
    "# Average scores\n",
    "avg_scores = {key: sum(values) / len(values) for key, values in scores.items()}\n",
    "print(\"\\nAverage ROUGE Scores on Test Set:\")\n",
    "for key, value in avg_scores.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # 20%,1300 -> 130minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25707d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\longt5\\lib\\site-packages\\transformers\\modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Summary:\n",
      " sunspot data from 1923 - 1933 (cycle 16) are analysed. a new method of the diagnosis of an echo-effect in the power spectrum is presented.\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.5185\n",
      "rouge2: 0.1538\n",
      "rougeL: 0.3704\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "\n",
    "text_to_summarize = \"\"\"\n",
    "In this paper the problem of the existence of the periodicity of about 155 days during the maximum activity period \n",
    "for sunspot data from 1923 - 1933 (cycle 16) is considered. The daily sunspot areas, the mean sunspot areas per \n",
    "Carrington rotation, the monthly sunspot numbers and their fluctuations, which are obtained after removing the 11-year \n",
    "cycle are analysed. A new method of the diagnosis of an echo-effect in the power spectrum is presented. Numerical results \n",
    "of the new method are presented.\n",
    "\"\"\"\n",
    "\n",
    "reference_summary = \"\"\"The paper explores the periodicity of approximately 155 days in sunspot activity during 1923â€“1933, using various data and a new diagnostic method for echo effects in power spectra.\"\"\"\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"./longt5_best_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./longt5_best_model\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess and generate\n",
    "input_text = \"summarize: \" + text_to_summarize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=4096, truncation=True).to(device)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=256,\n",
    "    min_length=30,\n",
    "    length_penalty=2.0,\n",
    "    repetition_penalty=1.2,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Summary:\\n\", generated_summary)\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "if reference_summary:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    score = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for k, v in score.items():\n",
    "        print(f\"{k}: {v.fmeasure:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longt5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
